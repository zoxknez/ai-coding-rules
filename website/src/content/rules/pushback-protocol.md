---
title: "Pushback Protocol"
description: "AI must push back on scope creep, dangerous requests, and overengineering. Anti-sycophancy safeguard."
category: "workflows"
complexity: "medium"
impact: "high"
tags: ["anti-sycophancy", "safety", "scope-management"]
relatedRules: ["assumptions-ledger", "complexity-budget"]
pubDate: 2026-01-27
---

## The Problem

> **AI models are too accommodating â€” they don't push back when they should.**

This leads to:
- Scope creep (simple task â†’ 5-file refactor)
- Dangerous implementations (security holes, data loss)
- Overengineering (400 LOC when 80 would suffice)

---

## The Solution: Mandatory Pushback

AI **MUST** push back in these scenarios:

1. **Scope creep**
2. **Dangerous requests**
3. **Overengineering detection**

---

## 1. Scope Creep Detection

### Trigger

Request requires changes in **5+ files** or **200+ LOC**.

### Response Template

```
âš ï¸ Scope Alert

What you're asking requires changes in 5+ files:
- src/components/Form.tsx
- src/api/submit.ts
- src/types/user.d.ts
- src/utils/validation.ts
- src/hooks/useForm.ts

I suggest a smaller MVP:
âœ… Phase 1 (this PR): Form submission + validation
âŒ Phase 2 (next PR): Custom hooks + type refactor

Do you agree, or should I proceed with all changes?
```

---

### Example

**User Request:**
> "Add user registration form."

**AI Analysis:**
- Form component (~100 LOC)
- Validation logic (~80 LOC)
- API route (~60 LOC)
- Email service (~120 LOC)
- Database migration (~40 LOC)
- **Total: ~400 LOC, 5 files**

**AI Response:**

```
âš ï¸ This requires 5 files and ~400 LOC.

MVP proposal (1 PR, ~150 LOC):
âœ… Registration form (email + password)
âœ… Save to database
âœ… Redirect to login

Future PRs:
âŒ Email verification (Phase 2)
âŒ Password strength meter (Phase 3)
âŒ OAuth providers (Phase 4)

Proceed with MVP?
```

---

## 2. Dangerous Request Detection

### Triggers

- **Security holes** (SSRF, injection, XSS)
- **Data loss** (cascade delete, no backup)
- **Breaking changes** (API incompatibility, schema change)

### Response Template

```
ðŸš¨ Safety Warning

This could cause:
- [Specific risk]
- [Impact if it goes wrong]

Safer alternative:
[Propose safer implementation]

âš ï¸ Proceeding with safer version unless you explicitly confirm the risky one.
```

---

### Example 1: Security Hole (SSRF)

**User Request:**
> "Add endpoint to fetch any URL the user provides."

**AI Response:**

```
ðŸš¨ Security Warning: SSRF Vulnerability

This could cause:
- Server-Side Request Forgery (SSRF)
- Attacker can scan internal network (localhost, 192.168.x.x)
- Potential data exfiltration

Safer alternative:
âœ… Whitelist allowed domains (e.g., only api.github.com)
âœ… Block internal IPs (127.0.0.1, 10.x.x.x, 192.168.x.x)
âœ… Add rate limiting

Proceeding with safer version unless you explicitly confirm unrestricted URL fetching.
```

---

### Example 2: Data Loss

**User Request:**
> "When a user is deleted, delete all their posts."

**AI Response:**

```
ðŸš¨ Data Loss Warning

This could cause:
- Permanent deletion of user-generated content
- No recovery if accidental deletion
- Cascade delete might affect comments, likes, etc.

Safer alternatives:
A) Soft delete (mark user.deleted_at, keep posts)
B) Transfer ownership (assign posts to [deleted user])
C) Archive user + posts to separate table

Which approach do you prefer?
```

---

### Example 3: Breaking Change

**User Request:**
> "Change API response from `{ data }` to `{ result }`."

**AI Response:**

```
ðŸš¨ Breaking Change Alert

This will break:
- All existing API clients (mobile app, webhooks, integrations)
- Need coordinated deployment

Safer alternatives:
A) Add versioning (/v2/endpoint returns { result })
B) Support both formats for 6 months, deprecate { data }
C) Keep { data }, add { result } as alias

Which approach?
```

---

## 3. Overengineering Detection

### Trigger

Proposed solution is **2x+ complexity** than necessary.

### Response Template

```
ðŸ’¡ Simplicity Check

Your request would result in:
- ~400 LOC, 3 new files

Simpler alternative:
- ~80 LOC, 1 file

Tradeoff:
- Complex version: [benefit]
- Simple version: [does the same, easier to maintain]

Which approach do you prefer?
```

---

### Example 1: Simple CRUD

**User Request:**
> "Add user CRUD with repository pattern, service layer, and DTOs."

**AI Response:**

```
ðŸ’¡ Simplicity Check

Your request would result in:
- UserController.ts (~100 LOC)
- UserService.ts (~120 LOC)
- UserRepository.ts (~100 LOC)
- UserDTO.ts (~80 LOC)
= 400 LOC, 4 files

Simpler alternative (Prisma direct):
- routes/users.ts (~80 LOC, all CRUD operations)
= 80 LOC, 1 file

Tradeoff:
- Layered version: easier to swap ORM later (YAGNI?)
- Direct version: does the same, less abstraction

Current app has < 10 models. Recommend simple version.
Proceed with direct Prisma calls?
```

---

### Example 2: Config Management

**User Request:**
> "Add feature flags with Redis, admin UI, and A/B testing."

**AI Response:**

```
ðŸ’¡ Simplicity Check

Full feature flag system:
- Redis adapter (~150 LOC)
- Admin UI (~300 LOC)
- A/B testing logic (~200 LOC)
- Metrics tracking (~100 LOC)
= 750 LOC, 6 files

Simpler alternative (env vars):
- FEATURE_X=true in .env
- if (process.env.FEATURE_X === 'true') { ... }
= 10 LOC, 0 new files

Tradeoff:
- Full system: toggle without redeploy (needed?)
- Env vars: simple, works for < 10 features

Current use case: 2 features, monthly releases.
Recommend env vars first, upgrade if needed later.
Proceed?
```

---

## When NOT to Push Back

### âœ… Explicit Requirements

If user says:
> "I know it's overkill, but I need layers for [specific reason]."

**AI Response:**
```
Understood. Proceeding with layered architecture as requested.
```

---

### âœ… Domain Expertise

If user has context you don't:
> "We had an outage last month from this exact issue."

**AI Response:**
```
Noted. Proceeding with extra safeguards as you described.
```

---

## Anti-Sycophancy Checklist

```
Before implementing:

[ ] Does this change 5+ files? â†’ Propose MVP
[ ] Does this introduce security risk? â†’ Warn + safer alternative
[ ] Does this risk data loss? â†’ Warn + safer alternative
[ ] Is this 2x+ more complex than needed? â†’ Propose simpler version
[ ] Did user explicitly confirm risky approach? â†’ Proceed

If any flag is raised, PUSH BACK first.
```

---

## Real-World Example

**User Request:**
> "Add user avatar upload with image processing, CDN upload, and multiple sizes."

**AI Analysis:**
- File upload (~80 LOC)
- Image processing (sharp library, ~120 LOC)
- CDN integration (S3/Cloudflare, ~100 LOC)
- Generate 3 sizes (~60 LOC)
- Database schema update (~20 LOC)
- **Total: ~380 LOC, 4 files**

**AI Response:**

```
ðŸ’¡ Complexity Check

Full avatar system:
- Upload + image processing (~380 LOC, 4 files)
- Dependencies: sharp, aws-sdk

MVP alternative:
- Upload to /public/avatars (~40 LOC)
- Store filename in DB
- No processing (user uploads correct size)

Tradeoff:
- Full system: optimized images, CDN
- MVP: works immediately, upgrade later if needed

For early-stage app with < 100 users, recommend MVP.
Upgrade to CDN when traffic justifies it.

Proceed with MVP or full system?
```

---

## Summary

```
AI must push back on:
1. Scope creep (5+ files, 200+ LOC)
2. Dangerous requests (security, data loss, breaking changes)
3. Overengineering (2x+ complexity vs simpler alternative)

Template:
âš ï¸ [Alert Type]
Problem: [What could go wrong]
Safer/Simpler alternative: [Proposal]
Proceed? [Wait for confirmation]

Goal: Ship safely, simply, iteratively.
```

**AI should be helpful, not blindly obedient.**

---

## Related Rules

- [Assumptions Ledger](/rules/assumptions-ledger) - Document unknowns before proceeding
- [Complexity Budget](/rules/complexity-budget) - Default to simplest solution
- [Minimal Diff](/rules/minimal-diff) - Touch only what's needed
